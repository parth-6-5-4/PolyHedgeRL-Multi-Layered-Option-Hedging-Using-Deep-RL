{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04870eef",
   "metadata": {},
   "source": [
    "# 03_Agent_Training\n",
    "\n",
    "Train a deep RL agent (PPO) in the custom multi-asset environment and monitor learning performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3436c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Imports\n",
    "from src.environment.Multi_asset_env import MultiAsset21DeepHedgingEnv\n",
    "from src.agents.ppo_agent import PPOAgent\n",
    "from src.utils.data_utils import download_market_data\n",
    "from src.environment.option_pricing import create_synthetic_option_chain\n",
    "from src.config.settings import get_config\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Prepare Data & Env\n",
    "df = download_market_data(**get_config('data'))\n",
    "opt_chain = create_synthetic_option_chain(df, get_config('option'))\n",
    "\n",
    "strikes = get_config('option')['strike_offsets']\n",
    "expiries = get_config('option')['expiry_days']\n",
    "types_ = get_config('option')['option_types']\n",
    "asset_universe = [{'strike_offset': s, 'expiry_days': e, 'type': t}\n",
    "                  for e in expiries for s in strikes for t in types_]\n",
    "\n",
    "env = MultiAsset21DeepHedgingEnv(df, opt_chain, asset_universe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990d244",
   "metadata": {},
   "source": [
    "## 3.3 PPO Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b66a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take some time (can set low timesteps for demo)\n",
    "agent = PPOAgent(env)\n",
    "model = agent.create_model()\n",
    "\n",
    "history = model.learn(total_timesteps=50000)\n",
    "model.save(\"results/models/ppo_polyhedge_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4699d852",
   "metadata": {},
   "source": [
    "## 3.4 Plotting Training Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2adbb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using wandb or TensorBoard, you can show the logs here.\n",
    "# Otherwise, just test the trained agent and show cumulative reward sample\n",
    "obs, _ = env.reset()\n",
    "rewards = []\n",
    "cum_pnl = []\n",
    "total = 0.0\n",
    "for _ in range(200):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    total += reward\n",
    "    cum_pnl.append(total)\n",
    "    if done: break\n",
    "\n",
    "plt.plot(cum_pnl)\n",
    "plt.title(\"Cumulative P&L after PPO Training\")\n",
    "plt.xlabel(\"Step\"); plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e8fcbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
