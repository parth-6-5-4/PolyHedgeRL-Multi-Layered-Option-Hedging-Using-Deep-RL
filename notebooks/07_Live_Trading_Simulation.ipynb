{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33da9f54",
   "metadata": {},
   "source": [
    "# 07_Live_Trading_Simulation\n",
    "\n",
    "Demonstrate the RL agent in a simulated real-time (on-the-fly) trading environment using live/fresh market data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.environment.Multi_asset_env import MultiAsset21DeepHedgingEnv\n",
    "from src.environment.option_pricing import create_synthetic_option_chain\n",
    "from src.config.settings import get_config\n",
    "\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561ffe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fetching new spot/option prices with each step (demo: use last N points)\n",
    "cfg = get_config('data')\n",
    "df_live = yf.download(cfg['symbol'], start='2024-01-01', end=cfg['end_date'], interval='1d').reset_index()\n",
    "df_live = df_live.rename(columns={'Date': 'date', 'Close': 'close'})\n",
    "option_chain_live = create_synthetic_option_chain(df_live, get_config('option'))\n",
    "\n",
    "strikes = get_config('option')['strike_offsets']\n",
    "expiries = get_config('option')['expiry_days']\n",
    "types_ = get_config('option')['option_types']\n",
    "asset_universe = [{'strike_offset': s, 'expiry_days': e, 'type': t}\n",
    "                  for e in expiries for s in strikes for t in types_]\n",
    "\n",
    "env = MultiAsset21DeepHedgingEnv(df_live, option_chain_live, asset_universe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e384fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your best model trained previously\n",
    "from stable_baselines3 import PPO\n",
    "model = PPO.load(\"results/models/ppo_polyhedge_agent\", env=env)   # Adjust path if needed\n",
    "\n",
    "obs, _ = env.reset()\n",
    "rewards, cum_pnl, actions = [], [], []\n",
    "total = 0.0\n",
    "\n",
    "for _ in range(50):  # Simulate 50 live steps\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, trunc, info = env.step(action)\n",
    "    rewards.append(reward)\n",
    "    total += reward\n",
    "    cum_pnl.append(total)\n",
    "    actions.append(action)\n",
    "    if done: break\n",
    "    time.sleep(0.1)  # Emulate live delay for demonstration\n",
    "\n",
    "plt.plot(cum_pnl)\n",
    "plt.title(\"Pseudo-Live Simulated Cumulative P&L\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840412b6",
   "metadata": {},
   "source": [
    "## 7.2 Inspect Agent's Actions Over Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8457c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_arr = np.array(actions)\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(actions_arr[:, 0], label='Spot')\n",
    "for i in range(1, 6):  # Plot first 5 option legs only for clarity\n",
    "    plt.plot(actions_arr[:, i], alpha=0.7, label=f'Opt{i}')\n",
    "plt.title(\"Positions (Spot + First 5 Options)\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Position size\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63b24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
